\documentclass{beamer}

\usepackage{lads}
\setbeamertemplate{navigation symbols}{}

\title{Eigenvectors and Eigenvalues}
\date{}
\author{BIOE 210}

\begin{document}

\maketitle

\begin{frame}
\frametitle{A curious case of matrix multiplication.}

\[ \VA = \begin{mex} 2 & 7 \\ -1 & -6\end{mex} \]

\pause
Let $\Vx_1 = \vectwo{-1}{1}$, then
\[ \VA\Vx_1 = \begin{mex} 2 & 7 \\ -1 & -6\end{mex}\vectwo{-1}{1} = \vectwo{5}{-5} 
= -5\vectwo{-1}{1} = -5\Vx_1 \]
\pause
or, let $\Vx_2 = \vectwo{-7}{1}$:
\[ \VA\Vx_2 = \begin{mex} 2 & 7 \\ -1 & -6\end{mex}\vectwo{-7}{1} = \vectwo{-7}{1} 
 = \Vx_2 \]
\end{frame}

\begin{frame}
\frametitle{The matrix \VA\ isn't special.}

\[ \VA = \begin{mex} 2 & 7 \\ -1 & -6\end{mex} \]

Let $\Vx_3 = \vectwo{1}{1}$.
\[ \VA\Vx_3 = \begin{mex} 2 & 7 \\ -1 & -6\end{mex}\vectwo{1}{1} = \vectwo{9}{5} 
 \ne \lambda\Vx_3 \]
	
\end{frame}

\begin{frame}
\frametitle{The vectors $\Vx_1$ and $\Vx_2$ aren't special.}	

\[ \VB = \begin{mex}2 & 1\\-3 & 0\end{mex} \]
\begin{align*}
	\VB\Vx_1 &= \begin{mex}2 & 1\\-3 & 0\end{mex}\vectwo{-1}{1} = \vectwo{-1}{3} \ne \lambda\Vx_1 \\
	\VB\Vx_2 &= \begin{mex}2 & 1\\-3 & 0\end{mex}\vectwo{-7}{1} = \vectwo{-13}{21} \ne \lambda\Vx_2
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Eigenvectors and eigenvalues}

The relationship between the matrix \VA\ and the vectors $\Vx_1$ and $\Vx_2$ is special. These vectors are \emph{eigenvectors} of the matrix.
\[ \VA\Vx = \lambda\Vx \]
The associated scalar $\lambda$ is called the \emph{eigenvalue}.
\end{frame}

\begin{frame}
\frametitle{Properties of eigenvectors and eigenvalues}
\begin{itemize}
	\item Only square matrices can have eigenvectors and eigenvalues.
	\item All eigenvectors for a matrix are linearly independent.
	\item An $n\times n$ matrix can have up to $n$ eigenvectors.
	\item All eigenvectors for a matrix are unique, but the eigenvalues can repeat.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The eigenbasis}

A matrix is \emph{perfect} if it has a complete set of eigenvectors. These eigenvectors form a basis called the \emph{eigenbasis}.

\bigskip
We can decompose vectors over the eigenbasis.
\[ \Vx = a_1\Vv_1 + \cdots + a_n\Vv_n \]
This simplifies matrix multiplication.
\begin{align*}
	 \VA\Vx &= \VA\left(a_1\Vv_1 + \cdots + a_n\Vv_n\right) \\
		&= a_1\VA\Vv_1 + \cdots + a_n\VA\Vv_n \\
		&= a_1\lambda_1\Vv_1 + \cdots + a_n\lambda_n\Vv_n
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Finding eigenvectors by the Power Method}

\[ \VA\Vx = a_1\lambda_1\Vv_1 + \cdots + a_n\lambda_n\Vv_n \]
\[ \VA^2\Vx = a_1\lambda_1^2\Vv_1 + \cdots + a_n\lambda_n^2\Vv_n \]
\[ \VA^k\Vx = a_1\lambda_1^k\Vv_1 + \cdots + a_n\lambda_n^k\Vv_n \]

\bigskip
\pause
As $k$ increases, the term with the largest $\lambda$ will overtake the sum. Thus
\[ \lim_{k\rightarrow\infty} \VA^k\Vx \propto \Vv_\mathrm{max} \]
\end{frame}

\begin{frame}
\frametitle{Applications}

Eigenvectors and eigenvalues have many applications in science and engineering.

\medskip
For this class, skip the sections on \emph{Solving Systems of ODEs} and \emph{Stability of Linear ODEs}. These topics overlap with other BIOE courses.

\medskip
Please read the sections on \emph{Positive Definite Matrices} and \emph{Network Centrality}.	
\end{frame}

\begin{frame}
\frametitle{The geometry of eigenvectors}

If we decompose a vector over an eigenbasis, matrix multiplication is akin to scaling each dimension by a constant (the eigenvalue).
\begin{align*}
	\Vx &= a_1\Vv_1 + a_2\Vv_2 \\
	\VA\Vx &= \VA(a_1\Vv_1 + a_2\Vv_2) \\
		&= \lambda_1 a_1\Vv_1 + \lambda_2 a_2\Vv_2
\end{align*}

\pause
The total ``volume" change after multiplying by a matrix is the product of the eigenvalues.
\[ \frac{\mathrm{volume}(\VA\Vx)}{\mathrm{volume}(\Vx)} = \frac{\lambda_1 a_1 \lambda_2 a_2 \lambda_3 a_3}{a_1 a_2 a_3} = \lambda_1\lambda_2\lambda_3 \]

\pause
This volume change is a characteristic of the matrix called the \emph{determinant}.
\[ \det(\VA) = \lambda_1\lambda_2\cdots\lambda_n \]
\end{frame}

\begin{frame}
\frametitle{Properties of the determinant}

Although we won't prove it in this course, {\bf a matrix has an inverse if and only if the determinant of the matrix is nonzero}. 

\pause
\bigskip
The following statements are equivalent for a square matrix \VA:
\begin{itemize}
	\item \VA\ can be transformed into the identity matrix by elementary row operations.
	\item The system $\VA\Vx = \Vy$ is solvable and has a unique solution.
	\item \VA\ is full rank.
	\item $\VA^{-1}$ exists.
	\item $\det(\VA) \ne 0$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Wrapping up the Field Axioms}
Using the determinant we can concisely state our last field axiom. Recall that for scalars we required a multiplicative inverse exist for any nonzero member of the field, i.e.
\begin{center}
	For all scalars $a\ne 0$ there exists $a^{-1}$ such that $aa^{-1} = 1$.
\end{center}
\pause
For vector spaces, we have the following:
\begin{center}
	For all square matrices \VA\ where $\det(\VA)\ne 0$,\\there exists $\VA^{-1}$ such that $\VA\VA^{-1} = \VI = \VA^{-1}\VA$.
\end{center}

\end{frame}




\end{document}
